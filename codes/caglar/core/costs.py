from commons import EPS
import theano.tensor as TT


def sparsity_penalty(h, sparsity_level=0.05, sparse_reg=1e-4):

    if h.ndim == 2:
        sparsity_level = TT.extra_ops.repeat(sparsity_level, h.shape[1])
    else:
        sparsity_level = TT.extra_ops.repeat(sparsity_level, h.shape[0])

    sparsity_penalty = 0
    avg_act = h.mean(axis=0)
    kl_div = kl_divergence(sparsity_level, avg_act)
    sparsity_penalty = sparse_reg * kl_div.sum()
    # Implement KL divergence here.
    return sparsity_penalty


def kl_simple(Y, Y_hat, cost_mask=None):
    """
    Warning: This function expects a sigmoid nonlinearity in the
    output layer. Returns a batch (vector) of mean across units of
    KL divergence for each example,
    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:
    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
    For binary p, some terms drop out:
    - p log q - (1-p) log (1-q)
    - p log sigmoid(z) - (1-p) log sigmoid(-z)
    p softplus(-z) + (1-p) softplus(z)
    Parameters
    ----------
    Y : Variable
        targets for the sigmoid outputs. Currently Y must be purely binary.
        If it's not, you'll still get the right gradient, but the
        value in the monitoring channel will be wrong.
    Y_hat : Variable
        predictions made by the sigmoid layer. Y_hat must be generated by
        fprop, i.e., it must be a symbolic sigmoid.
    -------
    ave : Variable
        average kl divergence between Y and Y_hat.
    """

    assert hasattr(Y_hat, 'owner')

    owner = Y_hat.owner
    assert owner is not None
    op = owner.op

    if not hasattr(op, 'scalar_op'):
        raise ValueError("Expected Y_hat to be generated by an Elemwise "
                         "op, got "+str(op)+" of type "+str(type(op)))

    assert isinstance(op.scalar_op, TT.nnet.sigm.ScalarSigmoid)

    z, = owner.inputs
    z = z.reshape(Y.shape)
    term_1 = Y * TT.nnet.softplus(-z)
    term_2 = (1 - Y) * TT.nnet.softplus(z)

    total = term_1 + term_2
    if cost_mask is not None:
        total = cost_mask * total

    ave = total.sum() / (total.shape[1] * (total.shape[2] - 2))
    #ave = ave.mean()
    return ave


def kl(Y, Y_hat, cost_mask=None, batch_vec=True, cost_matrix=False):
    """
    Warning: This function expects a sigmoid nonlinearity in the
    output layer. Returns a batch (vector) of mean across units of
    KL divergence for each example,
    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:
    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
    For binary p, some terms drop out:
    - p log q - (1-p) log (1-q)
    - p log sigmoid(z) - (1-p) log sigmoid(-z)
    p softplus(-z) + (1-p) softplus(z)
    Parameters
    ----------
    Y : Variable
        targets for the sigmoid outputs. Currently Y must be purely binary.
        If it's not, you'll still get the right gradient, but the
        value in the monitoring channel will be wrong.
    Y_hat : Variable
        predictions made by the sigmoid layer. Y_hat must be generated by
        fprop, i.e., it must be a symbolic sigmoid.
    -------
    ave : Variable
        average kl divergence between Y and Y_hat.
    """

    assert hasattr(Y_hat, 'owner')

    owner = Y_hat.owner
    assert owner is not None
    op = owner.op

    if not hasattr(op, 'scalar_op'):
        raise ValueError("Expected Y_hat to be generated by an Elemwise "
                         "op, got "+str(op)+" of type "+str(type(op)))

    assert isinstance(op.scalar_op, TT.nnet.sigm.ScalarSigmoid)

    z, = owner.inputs
    z = z.reshape(Y.shape)
    term_1 = Y * TT.nnet.softplus(-z)
    term_2 = (1 - Y) * TT.nnet.softplus(z)

    total = term_1 + term_2
    if cost_mask is not None:
        total = cost_mask * total

    if cost_matrix:
        ave = total.sum(-1) / TT.cast((total.shape[2] - 2), "float32")
    else:
        if batch_vec:
            ave = total.sum(0).sum(1) / TT.cast((total.shape[2] - 2), "float32")
        else:
            ave = total.sum() / (total.shape[1] * (total.shape[2] - 2))

    #ave = ave.mean()
    return ave


def _grab_probs(class_probs, target, use_fast_ver=False):
    shape0 = class_probs.shape[0]
    shape1 = class_probs.shape[1]

    p = None
    if target.ndim == 2 and use_fast_ver:
        target = target.flatten()
        cp = class_probs.reshape((target.shape[0], -1))
        p = TT.diag(cp.T[target])
    else:
        if target.ndim > 1:
            target = target.flatten()
        assert target.ndim == 1, 'make sure target is a vector of ints'
        assert 'int' in target.dtype
        pos = TT.arange(shape0)*shape1
        new_targ = target + pos
        #p = class_probs.flatten().dimshuffle(0, 'x')[new_targ, :]
        p = class_probs.reshape((shape0*shape1, 1))[new_targ].reshape((shape0,))
    return p


def nll_simple(Y, Y_hat, cost_mask=None):

    probs = Y_hat
    pred = TT.argmax(probs, axis=1).reshape(Y.shape)
    errors = TT.neq(pred, Y)

    LL = TT.log(_grab_probs(probs, Y) + 1e-8).reshape(Y.shape)

    if cost_mask is not None:
        total = cost_mask * LL
        errors = cost_mask * errors
        ncosts = TT.sum(cost_mask)
        mean_errors = TT.sum(errors) / (ncosts)
        ave = -TT.sum(total) / Y.shape[1]
    else:
        mean_errors = TT.mean(errors)
        ave = -TT.sum(LL) / Y.shape[0]
    return ave, mean_errors


def nll(Y, Y_hat, cost_mask=None, batch_vec=True, cost_matrix=False, use_fast_ver=False):
    probs = Y_hat
    pred = TT.argmax(probs, axis=-1).reshape(Y.shape)
    errors = TT.neq(pred, Y)
    LL = TT.log(_grab_probs(probs, Y, use_fast_ver=use_fast_ver) + 1e-8).reshape(Y.shape)

    if cost_mask is not None:
        total = cost_mask * LL
        errors = cost_mask * errors
        ncosts = TT.sum(cost_mask)
        mean_errors = TT.sum(errors) / (ncosts)
        if cost_matrix:
            ave = -total
        else:
            if batch_vec:
                ave = -total.sum(0)
            else:
                ave = -TT.sum(total) / Y.shape[1]
    else:
        mean_errors = TT.mean(errors)
        if cost_matrix:
            ave = -LL
        else:
            if batch_vec:
               ave = -LL.sum(0)
            else:
               ave = -TT.sum(LL) / Y.shape[1]

    return ave, mean_errors


def nll_hints(Y, Y_hat, npatches=64, cost_mask=None, cost_matrix=False, use_mask=False):
    if use_mask:
        mask = Y > 0
        mask = TT.set_subtensor(mask[:, -1], 1)
        mask = TT.cast(mask, "float32")

    if Y_hat.ndim == 3:
        probs = Y_hat.reshape((Y.shape[0]*Y.shape[1], -1))
    else:
        probs = Y_hat

    if Y.ndim == 2:
        targets = Y.reshape((Y.shape[0]*Y.shape[1],))
    else:
        targets = Y

    LL = TT.log(_grab_probs(probs, targets) + 1e-8).reshape((Y.shape[0], Y.shape[1], -1))

    if use_mask:
        LL = mask.dimshuffle(0, 1, 'x') * LL

    cost = -LL
    pred = TT.argmax(probs, axis=-1)
    errors = TT.neq(pred, targets)

    mean_errors = errors.mean()
    mean_cost = cost.mean(0).sum()
    return mean_cost, mean_errors


def huber_loss(y_hat, target, delta=1, center=0, std=1):

    l1_diff = abs((target - center - y_hat) / std)
    huber_loss = TT.switch(TT.ge(l1_diff, delta),
                           (2*l1_diff - 1) * delta,
                           l1_diff**2)
    return huber_loss

